{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9367cff-225f-41e2-9597-6b6c958a032f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data (X):\n",
      " [[0.37454012 0.95071431 0.73199394]\n",
      " [0.59865848 0.15601864 0.15599452]\n",
      " [0.05808361 0.86617615 0.60111501]\n",
      " [0.70807258 0.02058449 0.96990985]\n",
      " [0.83244264 0.21233911 0.18182497]\n",
      " [0.18340451 0.30424224 0.52475643]\n",
      " [0.43194502 0.29122914 0.61185289]\n",
      " [0.13949386 0.29214465 0.36636184]\n",
      " [0.45606998 0.78517596 0.19967378]\n",
      " [0.51423444 0.59241457 0.04645041]]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# 1. Import Libraries\n",
    "# ------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2. Generate Dummy Dataset (10 samples × 3 features)\n",
    "# ------------------------------------------------------\n",
    "\n",
    "np.random.seed(42)  # for reproducibility\n",
    "X = np.random.rand(10, 3)  # shape: (10, 3)\n",
    "print(\"Input Data (X):\\n\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63bc9b10-5596-4e2d-ab61-59ee046087c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights and Biases Initialized:\n",
      "W1:\n",
      " [[-1.15099358  0.37569802 -0.60063869 -0.29169375]\n",
      " [-0.60170661  1.85227818 -0.01349722 -1.05771093]\n",
      " [ 0.82254491 -1.22084365  0.2088636  -1.95967012]]\n",
      "b1:\n",
      " [[-1.32818605  0.19686124  0.73846658  0.17136828]]\n",
      "W2:\n",
      " [[-0.11564828]\n",
      " [-0.3011037 ]\n",
      " [-1.47852199]\n",
      " [-0.71984421]]\n",
      "b2:\n",
      " [[-0.46063877]]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# 3. Initialize Random Weights and Biases\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Hidden layer: 4 neurons → W1 shape (3, 4), b1 shape (1, 4)\n",
    "W1 = np.random.randn(3, 4)\n",
    "b1 = np.random.randn(1, 4)\n",
    "\n",
    "# Output layer: 1 neuron → W2 shape (4, 1), b2 shape (1, 1)\n",
    "W2 = np.random.randn(4, 1)\n",
    "b2 = np.random.randn(1, 1)\n",
    "\n",
    "print(\"\\nWeights and Biases Initialized:\")\n",
    "print(\"W1:\\n\", W1)\n",
    "print(\"b1:\\n\", b1)\n",
    "print(\"W2:\\n\", W2)\n",
    "print(\"b2:\\n\", b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dacde95d-5b6e-4c42-8503-c1347092a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# 4. Define the Sigmoid Activation Function\n",
    "# ------------------------------------------------------\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d8f966a-7377-4286-b5dc-c77636643ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# 5. Forward Propagation Steps\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Layer 1: Hidden Layer\n",
    "Z1 = np.dot(X, W1) + b1\n",
    "A1 = sigmoid(Z1)\n",
    "\n",
    "# Layer 2: Output Layer\n",
    "Z2 = np.dot(A1, W2) + b2\n",
    "A2 = sigmoid(Z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bc66582-aaf4-4b34-a7d9-e4a13735bf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hidden Layer Output (A1):\n",
      " [[0.151 0.769 0.658 0.085]\n",
      " [0.121 0.627 0.601 0.384]\n",
      " [0.194 0.748 0.694 0.126]\n",
      " [0.205 0.336 0.626 0.124]\n",
      " [0.094 0.664 0.568 0.342]\n",
      " [0.216 0.547 0.676 0.226]\n",
      " [0.183 0.538 0.646 0.188]\n",
      " [0.204 0.585 0.674 0.29 ]\n",
      " [0.103 0.829 0.621 0.234]\n",
      " [0.096 0.807 0.606 0.333]]\n",
      "\n",
      "Final Output (A2):\n",
      " [[0.149]\n",
      " [0.138]\n",
      " [0.139]\n",
      " [0.168]\n",
      " [0.147]\n",
      " [0.14 ]\n",
      " [0.15 ]\n",
      " [0.134]\n",
      " [0.141]\n",
      " [0.136]]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# 6. Print Forward Pass Outputs\n",
    "# ------------------------------------------------------\n",
    "\n",
    "print(\"\\nHidden Layer Output (A1):\\n\", np.round(A1, 3))\n",
    "print(\"\\nFinal Output (A2):\\n\", np.round(A2, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c3c3e-8aff-48fb-87f5-8c445457ed53",
   "metadata": {},
   "source": [
    "Forward propagation is the process of passing input data through the network\n",
    "layers using weights and biases to produce an output prediction.\n",
    "Each neuron applies a linear transformation followed by an activation function\n",
    "(sigmoid here) to introduce non-linearity, allowing the network to learn complex\n",
    "relationships. The hidden layer captures feature interactions, and the output\n",
    "layer produces the final prediction (between 0 and 1 in this case). This process\n",
    "is the foundation of how neural networks make decisions before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43f54975-c047-41e1-98c4-8f5e6cd295a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # for numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "840f6b1f-a9b3-484a-987e-d2f37bd6dfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Softmax Output (A2_softmax - class probabilities):\n",
      " [[0.771 0.129 0.1  ]\n",
      " [0.693 0.15  0.156]\n",
      " [0.772 0.132 0.096]\n",
      " [0.715 0.158 0.127]\n",
      " [0.7   0.145 0.155]\n",
      " [0.734 0.149 0.117]\n",
      " [0.73  0.148 0.122]\n",
      " [0.726 0.15  0.124]\n",
      " [0.746 0.132 0.122]\n",
      " [0.725 0.137 0.138]]\n",
      "\n",
      "One-hot Encoded Labels (Y_softmax):\n",
      " [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "Cross-Entropy Loss (using softmax output): 1.5449\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# 7.cross-entropy\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# For bonus, modify output layer to have 3 neurons (3 classes)\n",
    "W2_softmax = np.random.randn(4, 3)  # hidden layer size = 4\n",
    "b2_softmax = np.random.randn(1, 3)\n",
    "\n",
    "# Softmax activation function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # stability improvement\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Forward pass with softmax output\n",
    "Z2_softmax = np.dot(A1, W2_softmax) + b2_softmax\n",
    "A2_softmax = softmax(Z2_softmax)\n",
    "\n",
    "print(\"\\nSoftmax Output (A2_softmax - class probabilities):\\n\", np.round(A2_softmax, 3))\n",
    "\n",
    "# Create random one-hot encoded labels for 3 classes (for loss calculation)\n",
    "Y_softmax = np.eye(3)[np.random.choice(3, 10)]  # 10 samples, 3 classes\n",
    "\n",
    "# Cross-entropy loss for multi-class\n",
    "epsilon = 1e-9  # small value to avoid log(0)\n",
    "loss_softmax = -np.mean(np.sum(Y_softmax * np.log(A2_softmax + epsilon), axis=1))\n",
    "\n",
    "print(\"\\nOne-hot Encoded Labels (Y_softmax):\\n\", Y_softmax)\n",
    "print(\"Cross-Entropy Loss (using softmax output):\", round(loss_softmax, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a09665-bf4a-49bd-9527-06df3d99093c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
