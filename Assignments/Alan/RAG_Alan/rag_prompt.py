# -----------------------------
# Question:
# ----------------------------- 
# Assignment 3 ‚Äì RAG-Style Prompt Engineering 

# Goal:
# Simulate context retrieval + prompting (lightweight RAG).

# Tasks:
# 1. Take 5‚Äì6 paragraphs from any domain and save as context.

# 2. For 3 user questions, design prompts that:
#    - Insert retrieved context
#    - Ensure answers come ONLY from context
#    - If not found, respond: "Not found in context."

# 3. Evaluate the model:
#    - Hallucination?
#    - Accuracy?
#    - Impact of prompt changes

# Deliverables:
# - Context file
# - 3 questions
# - Prompt + answer table
# - Short reflection: How prompt design reduces hallucination




# =============================================================================================
# ----------------------------- 
# Assignment 3 ‚Äì RAG-Style Prompt Engineering 
# Streamlit UI Implementation
# -----------------------------

import streamlit as st
import pandas as pd
import json
from openai import OpenAI
import io

# ----------------------------
# PAGE CONFIG
# ----------------------------
st.set_page_config(
    page_title="RAG-Style Prompt Engineering",
    page_icon="ü§ñ",
    layout="wide"
)

# ----------------------------
# LLM SETUP
# ----------------------------
@st.cache_resource
def get_llm_client():
    base_url = "http:...."
    api_key = "...."
    
    return OpenAI(
        api_key=api_key,
        base_url=base_url
    )

client = get_llm_client()

# ----------------------------
# DEFAULT CONTEXT
# ----------------------------
DEFAULT_CONTEXT = """Climate change refers to long-term shifts in global temperatures and weather patterns. Since the 1800s, human activities have been the main driver of climate change, primarily due to burning fossil fuels like coal, oil, and gas.

The greenhouse effect is a natural process where certain gases trap heat in Earth's atmosphere. However, human activities have increased concentrations of greenhouse gases like carbon dioxide, methane, and nitrous oxide, intensifying this effect and causing global warming.

The global average temperature has increased by approximately 1.1¬∞C since pre-industrial times. The Intergovernmental Panel on Climate Change (IPCC) warns that exceeding 1.5¬∞C of warming could lead to severe and irreversible impacts on ecosystems and human societies.

Rising sea levels are one of the most visible impacts of climate change. Thermal expansion of water and melting ice sheets contribute to sea level rise, which threatens coastal communities and small island nations with flooding and erosion.

Extreme weather events such as hurricanes, droughts, heatwaves, and heavy rainfall have become more frequent and intense due to climate change. These events cause significant economic damage, displace populations, and threaten food and water security.

International agreements like the Paris Agreement aim to limit global warming by reducing greenhouse gas emissions. Countries have committed to achieving net-zero emissions by 2050, though current policies are insufficient to meet this goal without urgent action."""

# ----------------------------
# PROMPT TEMPLATES
# ----------------------------
def strict_rag_prompt(question, context):
    return f"""You are a precise information retrieval assistant.

CONTEXT:
{context}

INSTRUCTIONS:
- Answer the question ONLY using information from the CONTEXT above
- Do NOT use any external knowledge or make assumptions
- If the answer cannot be found in the context, respond EXACTLY with: "Not found in context."
- Be concise and accurate

QUESTION: {question}

ANSWER:"""

def medium_rag_prompt(question, context):
    return f"""Based on the following context, answer the question.

Context: {context}

Question: {question}

If the information is not in the context, say "Not found in context."

Answer:"""

def loose_prompt(question, context):
    return f"""Context: {context}

Question: {question}

Answer:"""

# ----------------------------
# LLM CALL FUNCTION
# ----------------------------
def call_llm(prompt, model="gpt-5.1", temperature=0):
    """Call LLM with given prompt"""
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error: {str(e)}"

# ----------------------------
# MAIN UI
# ----------------------------
st.title("ü§ñ RAG-Style Prompt Engineering Assignment")
st.markdown("### Simulate context retrieval + prompting (lightweight RAG)")

# ----------------------------
# SIDEBAR CONFIGURATION
# ----------------------------
with st.sidebar:
    st.header("‚öôÔ∏è Configuration")
    
    # Model selection
    model = st.selectbox(
        "Select Model",
        ["gpt-5.1"],
        index=0
    )
    
    temperature = st.slider(
        "Temperature",
        min_value=0.0,
        max_value=1.0,
        value=0.0,
        step=0.1,
        help="Higher values = more creative, lower = more deterministic"
    )
    
    st.divider()
    # st.header("üìö About")
    # st.markdown("""
    # **Assignment Goals:**
    # - Simulate RAG with context
    # - Test prompt engineering
    # - Evaluate hallucination
    # - Compare prompt designs
    # """)

# ----------------------------
# TABS
# ----------------------------
tab1, tab2, tab3, tab4 = st.tabs([
    "üìù 1. Context & Questions", 
    "üî¨ 2. Run Evaluation", 
    "üìä 3. Results", 
    "üí≠ 4. Reflection"
])

# ----------------------------
# TAB 1: CONTEXT & QUESTIONS
# ----------------------------
with tab1:
    st.header("Step 1: Define Context and Questions")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("üìÑ Context (5-6 paragraphs)")
        context = st.text_area(
            "Enter your context paragraphs",
            value=DEFAULT_CONTEXT,
            height=400,
            help="Paste 5-6 paragraphs from any domain"
        )
        
        if context:
            word_count = len(context.split())
            para_count = len([p for p in context.split('\n\n') if p.strip()])
            st.info(f"üìä {para_count} paragraphs, {word_count} words")
    
    with col2:
        st.subheader("‚ùì Questions")
        
        st.markdown("**Question 1** (answerable from context):")
        q1 = st.text_input(
            "Q1",
            value="What is the main cause of climate change since the 1800s?",
            label_visibility="collapsed"
        )
        
        st.markdown("**Question 2** (answerable from context):")
        q2 = st.text_input(
            "Q2",
            value="How much has the global average temperature increased?",
            label_visibility="collapsed"
        )
        
        st.markdown("**Question 3** (NOT in context - tests hallucination):")
        q3 = st.text_input(
            "Q3",
            value="What is the global population in 2025?",
            label_visibility="collapsed"
        )
        
        st.info("üí° Use Q3 to test if the model hallucinates when info is missing")
    
    # Store in session state
    if st.button("‚úÖ Save Context & Questions", type="primary"):
        st.session_state['context'] = context
        st.session_state['questions'] = [q1, q2, q3]
        st.success("‚úÖ Saved! Go to 'Run Evaluation' tab")

# ----------------------------
# TAB 2: RUN EVALUATION
# ----------------------------
with tab2:
    st.header("Step 2: Run RAG Evaluation")
    
    if 'context' not in st.session_state:
        st.warning("‚ö†Ô∏è Please save context and questions in Tab 1 first")
    else:
        st.success("‚úÖ Context and questions loaded")
        
        # Show prompt templates
        with st.expander("üîç View Prompt Templates"):
            st.markdown("**Strict RAG Prompt** (prevents hallucination):")
            st.code(strict_rag_prompt("QUESTION", "CONTEXT"), language="text")
            
            st.markdown("**Medium RAG Prompt:**")
            st.code(medium_rag_prompt("QUESTION", "CONTEXT"), language="text")
            
            st.markdown("**Loose Prompt** (allows hallucination):")
            st.code(loose_prompt("QUESTION", "CONTEXT"), language="text")
        
        if st.button("üöÄ Run Evaluation", type="primary"):
            context = st.session_state['context']
            questions = st.session_state['questions']
            
            results = []
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            total_tests = len(questions) * 3  # 3 prompt types
            current = 0
            
            for q_idx, question in enumerate(questions, 1):
                status_text.text(f"Processing Question {q_idx}: {question}")
                
                # Test each prompt template
                templates = [
                    ("Strict RAG", strict_rag_prompt),
                    ("Medium RAG", medium_rag_prompt),
                    ("Loose", loose_prompt)
                ]
                
                for template_name, template_func in templates:
                    prompt = template_func(question, context)
                    answer = call_llm(prompt, model=model, temperature=temperature)
                    
                    # Check for hallucination
                    hallucination = "Not found in context" not in answer and \
                                   question == questions[2]  # Q3 is not in context
                    
                    results.append({
                        "Question #": q_idx,
                        "Question": question,
                        "Prompt Type": template_name,
                        "Answer": answer,
                        "Hallucination?": "Yes ‚ö†Ô∏è" if hallucination else "No ‚úÖ",
                        "Full Prompt": prompt
                    })
                    
                    current += 1
                    progress_bar.progress(current / total_tests)
            
            st.session_state['results'] = results
            status_text.text("‚úÖ Evaluation complete!")
            progress_bar.progress(1.0)
            
            st.success("‚úÖ Results ready! Check the 'Results' tab")

# ----------------------------
# TAB 3: RESULTS
# ----------------------------
with tab3:
    st.header("Step 3: View Results")
    
    if 'results' not in st.session_state:
        st.info("üìä Run evaluation first to see results")
    else:
        results = st.session_state['results']
        df = pd.DataFrame(results)
        
        # Summary metrics
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Total Tests", len(results))
        
        with col2:
            hallucinations = df[df["Hallucination?"] == "Yes ‚ö†Ô∏è"].shape[0]
            st.metric("Hallucinations", hallucinations, delta=f"-{len(results)-hallucinations} accurate")
        
        with col3:
            accuracy = ((len(results) - hallucinations) / len(results)) * 100
            st.metric("Accuracy", f"{accuracy:.1f}%")
        
        st.divider()
        
        # Results by question
        for q_num in df["Question #"].unique():
            with st.expander(f"üìå Question {q_num}: {df[df['Question #']==q_num]['Question'].iloc[0]}"):
                q_df = df[df["Question #"] == q_num][["Prompt Type", "Answer", "Hallucination?"]]
                
                for idx, row in q_df.iterrows():
                    # Just show the answer without the prompt type label
                    st.write(row['Answer'])
                    if idx < len(q_df) - 1:  # Don't add divider after last answer
                        st.divider()
        
        # Full results table
        st.subheader("üìä Complete Results Table")
        display_df = df[["Question #", "Question", "Prompt Type", "Answer", "Hallucination?"]]
        st.dataframe(display_df, use_container_width=True, height=400)
        
        
        # Download options
        st.divider()
        st.subheader("üíæ Download Results")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # CSV download
            csv_buf = io.StringIO()
            df.to_csv(csv_buf, index=False)
            st.download_button(
                "üì• Download CSV",
                csv_buf.getvalue(),
                "rag_results.csv",
                "text/csv"
            )
        
        with col2:
            # TXT download
            txt_buf = io.StringIO()
            txt_buf.write("RAG EVALUATION RESULTS\n")
            txt_buf.write("=" * 80 + "\n\n")
            
            for _, row in df.iterrows():
                txt_buf.write(f"Question {row['Question #']}: {row['Question']}\n")
                txt_buf.write(f"Prompt Type: {row['Prompt Type']}\n")
                txt_buf.write(f"Answer: {row['Answer']}\n")
                txt_buf.write(f"Hallucination: {row['Hallucination?']}\n")
                txt_buf.write("-" * 80 + "\n\n")
            
            st.download_button(
                "üì• Download TXT",
                txt_buf.getvalue(),
                "rag_results.txt",
                "text/plain"
            )
        
        with col3:
            # JSON download
            json_str = json.dumps(results, indent=2)
            st.download_button(
                "üì• Download JSON",
                json_str,
                "rag_results.json",
                "application/json"
            )

# ----------------------------
# TAB 4: REFLECTION
# ----------------------------
with tab4:
    st.header("Step 4: Reflection & Analysis")
    
    st.markdown("""
    ### üí≠ How Prompt Design Reduces Hallucination
    
    #### üîë Key Findings:
    
    **1. Strict Constraints Prevent Hallucination**
    - Explicit instructions to ONLY use context prevent the model from generating information not present
    - Clear fallback message ("Not found in context.") forces honesty when information is unavailable
    - Works best for factual, retrieval-focused tasks
    
    **2. Prompt Structure Impact**
    """)
    
    comparison_data = {
        "Prompt Type": ["Strict RAG", "Medium RAG", "Loose"],
        "Hallucination Risk": ["Very Low üü¢", "Medium üü°", "High üî¥"],
        "Accuracy": ["Highest", "Medium", "Varies"],
        "Use Case": ["Factual QA", "General purpose", "Creative tasks"]
    }
    
    st.table(pd.DataFrame(comparison_data))
    
    st.markdown("""
    **3. Temperature Settings**
    - Temperature = 0: Deterministic, factual (best for RAG)
    - Temperature > 0.5: More creative but higher hallucination risk
    
    **4. Evaluation Insights**
    - Questions 1-2: Answerable from context ‚Üí Strict prompt gives accurate answers
    - Question 3: Not in context ‚Üí Strict prompt correctly returns "Not found"
    - Loose prompts may generate plausible but incorrect answers for Q3
    
    #### ‚úÖ Best Practices:
    
    1. **Be Explicit**: Tell the model exactly what to do and what NOT to do
    2. **Provide Boundaries**: Clear instructions about using only provided context
    3. **Define Fallbacks**: Specify exact response when information is missing
    4. **Use Low Temperature**: For factual tasks, keep temperature at 0
    5. **Test Edge Cases**: Include questions NOT in context to test hallucination
    
    #### üéØ Conclusion:
    
    Careful prompt engineering with explicit constraints, clear instructions, and structured 
    context presentation **significantly reduces hallucination**. The key is making the model's 
    boundaries explicit and providing clear fallback behavior.
    
    The **Strict RAG prompt** demonstrated the best performance in preventing hallucination 
    while maintaining accuracy for answerable questions.
    """)
    
    # Add downloadable reflection
    reflection_text = """
REFLECTION: How Prompt Design Reduces Hallucination
=" * 80

KEY FINDINGS:

1. STRICT CONSTRAINTS PREVENT HALLUCINATION
   - Explicit instructions to ONLY use context prevent the model from generating
     information not present in the source material
   - Clear fallback message ("Not found in context.") forces honesty when 
     information is unavailable

2. PROMPT STRUCTURE MATTERS
   - Strict RAG prompt: Highest accuracy, no hallucination
   - Medium RAG prompt: Some hallucination risk, less precise
   - Loose prompt: Highest hallucination risk, model uses external knowledge

3. TEMPERATURE SETTING
   - Temperature=0 ensures deterministic, factual responses
   - Higher temperatures increase creativity but also hallucination risk

4. EVALUATION RESULTS
   - Questions 1-2: Answerable from context, strict prompt gives accurate answers
   - Question 3: Not in context, strict prompt correctly returns "Not found"
   - Loose prompts may generate plausible but incorrect answers for Q3

BEST PRACTICES:
- Be explicit in instructions
- Provide clear boundaries
- Define fallback responses
- Use low temperature for factual tasks
- Test with edge cases

CONCLUSION:
Careful prompt engineering with explicit constraints significantly reduces hallucination.
The Strict RAG prompt demonstrated best performance in preventing hallucination while
maintaining accuracy.
"""
    
    st.download_button(
        "üì• Download Reflection",
        reflection_text,
        "reflection.txt",
        "text/plain"
    )

# ----------------------------
# FOOTER
# ----------------------------
st.divider()
st.markdown("""
<div style='text-align: center; color: gray;'>
    <p>Assignment 3 ‚Äì RAG-Style Prompt Engineering | Streamlit Demo</p>
</div>
""", unsafe_allow_html=True)
